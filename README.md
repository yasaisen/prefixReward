# prefixReward
do prefix tuning on bert-based reward for downstream gemma-7b tuning

## How to use
```shell
python prefixReward_v11_train.py --cfg-path projects/prefixReward_v11/train.yaml
```

## Model info
### prefixReward v11
#### inference
![](https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_inference.png){style="display: block; margin: 0 auto;"}

#### model structure
![](https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_model.png){style="display: block; margin: 0 auto;"}

#### training
![](https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_training.png){style="display: block; margin: 0 auto;"}


