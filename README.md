# prefixReward
do prefix tuning on bert-based reward for downstream gemma-7b tuning

## How to use
```shell
python prefixReward_v11_train.py --cfg-path projects/prefixReward_v11/train.yaml
```

## Model info
### prefixReward v11
#### inference
<div align="center">
  <img src="https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_inference.png" alt="inference" width="300">
</div>

#### model structure
<div align="center">
  <img src="https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_model.png" alt="model structure" width="400">
</div>

#### training
<div align="center">
  <img src="https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_training.png" alt="training" width="600">
</div>

