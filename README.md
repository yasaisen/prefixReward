# prefixReward
do prefix tuning on bert-based reward for downstream gemma-7b tuning

## How to use
```shell
python prefixReward_v11_train.py --cfg-path projects/prefixReward_v11/train.yaml
```

## Model info
### prefixReward v11
#### inference
<center>
  <img src="https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_inference.png" alt="inference" width="300">
</center>

#### model structure
<center>
  <img src="https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_model.png" alt="model structure" width="300">
</center>

#### training
<center>
  <img src="https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_training.png" alt="training" width="300">
</center>

