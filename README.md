# prefixReward
do prefix tuning on bert-based reward for downstream gemma-7b tuning

# How to use
```shell
python prefixReward_v11_train.py --cfg-path projects/prefixReward_v11/train.yaml
```

# Model info
## prefixReward v11
### inference
![](https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_inference.png)

### model structure
![](https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_model.png)

### training
![](https://github.com/yasaisen/prefixReward/blob/main/doc/prefixReward_v11/prefixReward_v11_training.png)


